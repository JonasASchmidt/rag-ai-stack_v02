# ==============================================
# RAG AI STACK CONFIGURATION
# ==============================================

# CORE DIRECTORIES
DOCS_DIR=docs
INDEX_DIR=storage

# OLLAMA LLM CONFIGURATION
# F端r lokale Entwicklung:
OLLAMA_API_URL=http://localhost:11434
# F端r Docker:
# OLLAMA_API_URL=http://ollama:11434

LLM_MODEL=llama3.1:latest
LLM_REQUEST_TIMEOUT=120

# OLLAMA ADVANCED OPTIONS
OLLAMA_KEEP_ALIVE=5m
OLLAMA_NUM_CTX=4096
OLLAMA_NUM_BATCH=512
OLLAMA_NUM_PREDICT=256

# DOCUMENT PROCESSING
CHUNK_SIZE=512
CHUNK_OVERLAP=50

# EMBEDDING CONFIGURATION  
EMBED_DIM=384

# RETRIEVAL SETTINGS
RETRIEVAL_K=5
FETCH_K=10

# MODEL LIMITS
MAX_INPUT_SIZE=4096
NUM_OUTPUT=512

# RESPONSE GENERATION
RESPONSE_MODE=tree_summarize
THINKING_STEPS=3
TEMPERATURE=0.1

# FILE WATCHING
DEBOUNCE_SECONDS=2

# PERFORMANCE TUNING
# F端r bessere Performance bei lokaler Entwicklung:
CHAINLIT_HOST=localhost
CHAINLIT_PORT=8000
CHAINLIT_AUTO_RELOAD=false

# LOGGING
LOG_LEVEL=INFO
ENABLE_DEBUG=false

# MEMORY OPTIMIZATION
# F端r begrenzte Ressourcen:
# OLLAMA_NUM_CTX=2048
# OLLAMA_NUM_BATCH=256
# CHUNK_SIZE=256
# RETRIEVAL_K=3