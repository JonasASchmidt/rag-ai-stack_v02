# Paths
DOCS_DIR=docs/
INDEX_DIR=vectorstore/llama

# Chunking / PromptHelper
CHUNK_SIZE=800
CHUNK_OVERLAP=0.1
MAX_INPUT_SIZE=4096
NUM_OUTPUT=512

# LLM / Ollama
LLM_MODEL=llama3.1:latest
# When using the Docker setup the Ollama service is reachable at
# http://ollama:11434.  For local development without Docker change this to
# http://localhost:11434.
OLLAMA_API_URL=http://ollama:11434
LLM_REQUEST_TIMEOUT=120
OLLAMA_KEEP_ALIVE=5m
OLLAMA_NUM_CTX=2048
OLLAMA_NUM_BATCH=16
OLLAMA_NUM_PREDICT=512

# Embeddings
EMBED_DIM=256

# Retrieval / Thinking knobs
RETRIEVAL_K=10
FETCH_K=30
RESPONSE_MODE=tree_summarize
THINKING_STEPS=2
TEMPERATURE=0.1
